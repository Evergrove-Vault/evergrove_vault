---
tags:
  - evergrow/bud
---

# Reciprocal Rank Fusion (RRF)
Предположим, у нас есть несколько списков документов (каждый — результат отдельного запроса).
- У документа в каждом списке есть **позиция (rank)**.
- RRF даёт документу **балл (score)**, который обратно пропорционален его позиции.

Формула:

$$
RRF(d)= \sum_{q \in Q} \frac{1}{k + rank_q(d)}
$$
​
где:
- d - документ,
- Q - набор запросов (или поисковых систем),
- $rank_q(d)$ - позиция документа в списке по запросу q,
- k - константа (обычно 60), чтобы не переоценивать топ-1 позиции.

# Weighted RRF (wRRF)
Модификация RRF, где вес разных источников (BM25, dense, hybrid) можно регулировать.

$$
wRRF(d)=\sum_{q \in Q} \frac{w_q}{k + rank_q(d)}​​
$$

> [!tip] Полезно
> Eсли мы знаем, что одна система надёжнее другой.

# Max/Min/Avg Score Fusion
Если поисковые системы возвращают **скоры**, можно:
- брать **максимальный** (оптимистично: если хотя бы одна система считает документ важным),
- **средний** (сбалансировано),
- **минимальный** (пессимистично).

# Learning to Rank (LTR)

## Основная идея
В традиционном поиске или рекомендациях у нас есть:
- **Запрос (query)** — то, что ищет пользователь.
- **Документы (items)** — объекты, которые могут быть релевантными запросу.
- **Релевантность (relevance)** — оценка того, насколько документ подходит под запрос (например, по кликам, оценкам пользователей или экспертной разметке).

LTR использует **машинное обучение**, чтобы на основе признаков документов и запроса предсказать **правильный порядок сортировки**.
## Виды подходов LTR

1. **Pointwise (по точке)**
    - Рассматривает каждый документ отдельно.
    - Модель предсказывает **оценку релевантности** документа для запроса.
    - Пример: регрессия, классификация (релевантен/не релевантен).
    - Минус: не учитывает взаимное сравнение документов.
2. **Pairwise (по парам)**
    - Рассматривает **пары документов** для одного запроса.
    - Модель учится **предпочитать один документ другому**.
    - Цель: минимизировать количество ошибок при сравнении пар.
    - Пример: ==RankNet, RankSVM.==
3. **Listwise (по списку)**
    - Учитывает **весь список документов сразу**.
    - Модель оптимизирует метрики ранжирования напрямую (например, NDCG, MAP).
    - Пример: ==LambdaMART, ListNet.==

# Cross-Encoder Re-Ranking
Очень популярный в современных RAG-системах 
- Берём **bi-encoder** (обычный векторный поиск) для быстрого candidate retrieval.
- Потом берём **cross-encoder** (BERT/DeBERTa), который смотрит на **запрос + документ вместе** и выдаёт точный релевантность-скор.  


> [!warning] Применять осторожно
> Дороже по вычислениям, но значительно повышает precision.  
> Пример энкодера: `[MS MARCO cross-encoders](https://huggingface.co/cross-encoder).

> [!info]- Би-энкодер VS кросс-энкодер
>  ## Би-энкодер
Делаем вектор вопроса и вектор каждой части текста отдельно. Считаем расстояния, выбираем наименьшее. Это би-энкодер. Назван так, потому, что для вычисления расстояния мы делаем два (би) вектора, а затем вычисляем косинусное расстояние между ними.
>
> ## Кросс-энкодер
А что если в модель подавать сразу и вопрос и ответ? Вообще, для модели это проще, она одновременно видит обе части и должна сказать нам, насколько вероятно, что это вопрос-ответ. У берт-подобных моделей это похоже на - NSP (next sentense perdiction). Эта задача даже использовалась на стадии pretrain таких моделей. Т.е. качество результат должно быть лучше.
Однако скорость работы сильно меньше. В нейросеть мы запускаем все пары, по каждой паре отдельно определяем вероятность.

---

# Self-Rerank (LLM-as-a-Ranker)
Используем сам LLM, чтобы он:
- прочитал список кандидатов,
- оценил их релевантность к запросу,
- выдал отсортированный список.

> [!warning] Применять осторожно
> 👉 Гибко (LLM умеет reasoning), но дорого и не всегда стабильно.

---

# Embedding-based Re-Ranking
Вместо одного similarity score можно:
## max similarity
считать **max similarity** между токенами (ColBERT, Late Interaction).
### Проблема классических dense retrieval моделей
- Обычно делаем так: **один вектор на документ** и **один вектор на запрос**, сравниваем косинусным расстоянием.
- Минус: документ может быть длинным, и один вектор усредняет много информации → теряется нюанс.

### Как работает ColBERT (2020)
- Документ разбивается на токены → каждый токен получает embedding (через BERT).
- Запрос тоже разбивается на токены, каждый токен получает embedding.
- При сравнении:

$$
Score(q,d) = \sum_{t \in q}\max_{w \in d}Sim(E(t), E(w))
$$

где:
- E(t) — embedding токена запроса,
- E(w) — embedding токена документа.

👉 То есть для каждого токена запроса мы ищем **самый близкий токен документа**, и берём их сумму.

> [!hint]+ Логика работы
> #### Интуиция:
> - Если в документе есть хотя бы один «правильный» токен, он сработает.
> - Это похоже на **soft keyword search**, но через dense embeddings.
    
> [!tip]+ Полезно
> - Отлично работает, когда важны **ключевые слова и нюансы формулировки** (например, QA).
> - Используется в **[RAGatouille](https://github.com/AnswerDotAI/RAGatouille)** и других продвинутых retrieval системах.
## multiple vector representations
использовать **multiple vector representations** (например, SPLADE — sparse + dense hybrid).

### Проблема:
- Dense модели (BERT-эмбеддинги) → хороши в семантике, но плохо ловят **точные совпадения слов**.
- Sparse модели (BM25, TF-IDF) → хороши в точных совпадениях, но не понимают синонимов.

### Решение: **SPLADE (Sparse Lexical And Dense)**
- Модель, обученная так, чтобы выдавать _многомерное распределение значимости токенов_.
- То есть вместо одного вектора мы получаем что-то вроде «супер-BM25», где веса слов вычислены нейросетью.

### Как это работает:
- Вектор документа получается разреженным (sparse) → в каждой координате стоит вес, соответствующий слову.
- Одновременно можно хранить и **dense представление** документа.
- При поиске:
    - часть скоринга идёт по sparse (BM25-style),
    - часть — по dense (semantic embeddings),
    - итоговый score = линейная комбинация или обученный fusion.
        

Формула:

$$
Score(q,d)=α⋅Sim_{dense}(q,d)+(1−α)⋅Sim_{sparse}(q,d)
$$

### Применение:
> [!tip]+ Полезно
> - Очень сильный baseline: на многих бенчмарках SPLADE обгоняет чисто dense retrieval.
> - В RAG → даёт **и точные термины, и семантику**.
> - поддерживается [[Qdrant]]
> - [SPLADE framework](https://github.com/naver/splade)

---

# MMR (Maximal Marginal Relevance)
Балансируем **релевантность** и **разнообразие**
Для документа $d_i$​ из множества кандидатов D, который мы хотим добавить в итоговый список S:

$$
MMR(d_{i})=λ⋅Sim(d_{i},q)−(1−λ)⋅max​_{d_{j} \in S}Sim(d_{i},d_{j})
$$

где:
- q — запрос,
- $d_{i}$​ — кандидатный документ,
- S — уже выбранные документы (результат на текущем шаге),
- $Sim(d_{i},q)$ — мера похожести документа на запрос (например, косинусная близость в эмбеддингах),
- $Sim(d_{i},d_{j})$ — мера похожести документа на уже выбранные документы,
- $λ∈[0,1]$ — коэффициент, определяющий баланс:
    - если $λ→1$ → упор на релевантность,
    - если $λ→0$ → упор на разнообразие

Документы, похожие на запрос, но **разные между собой**, поднимаются выше.  

> [!tip] Полезно
> 👉 Полезно, чтобы не тащить дубликаты и охватить разные аспекты вопроса

---

# Voting Methods

## Borda Count
Смысл: каждый ранжированный список (выданный retrieval-моделью) даёт **очки документам** в зависимости от позиции.

Если у нас n документов и документ $d_{i}$ стоит на позиции p (чем ниже p, тем лучше), то его очки:

$$
score(d_i)=n−p
$$

Затем:
- суммируем очки по всем системам,
- сортируем документы по суммарному score.

## Condorcet Voting
- Для каждой пары документов $d_i$, $d_j$​ смотрим, кто из них выше в каждом ранжированном списке.
- Документ, который чаще побеждает в попарных сравнениях, считается лучше.
---

# Neural Rankers с Fine-Tuning
- Обучаем **специализированную модель ранжирования** под свою доменную базу
- Можно взять BERT/DeBERTa + LoRA → fine-tune на парных данных (запрос, релевантный документ, нерелевантный документ).